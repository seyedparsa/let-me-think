model_type: mistral
# vocab_size=len(tokenizer),
# bos_token_id=tokenizer.bos_token_id,
# eos_token_id=tokenizer.eos_token_id,
# pad_token_id=tokenizer.pad_token_id,
hidden_size: &hidden_size 256
intermediate_size: *hidden_size
num_hidden_layers: 4
num_attention_heads: &num_heads 8
num_key_value_heads: *num_heads
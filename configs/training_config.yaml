seed: 42
teach: dfs
num_train: 100000
num_epochs: 100 
num_node_tokens: 102
context_length: 1000
batch_size: 500
eval_steps: 100
log_steps: 100
save_steps: 500
gradient_accumulation_steps: 1
weight_decay: 0.1
warmup_steps: 2000
lr: 0.0001
lr_scheduler_type: cosine
save_total_limit: 5
model_config: configs/model_config.yaml
output_dir: output_dir
data_dir: data_dir
train_file: train_comblockd10-w5_decision_n100000.json
val_file: val_comblockd10-w5_decision_n1000.json
wandb:
  project: bepatient
  entity: seyedparsa
  dir: wandb

